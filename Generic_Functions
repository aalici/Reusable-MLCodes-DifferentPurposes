import warnings

warnings.filterwarnings("ignore")
###for oracle connections###
import os
# import cx_Oracle as cx_Oracle
#####
import time
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from statsmodels.stats.stattools import medcouple
import scipy.stats as stats
from sklearn.metrics import roc_curve, precision_recall_curve, roc_auc_score, accuracy_score, classification_report, \
    confusion_matrix, precision_recall_curve, auc, make_scorer, recall_score, precision_score
#####
import pickle


def f_pickle_dump(v_file_name, v_tuple):
    pickle.dump(v_tuple, open(v_file_name, "wb"))


def f_pickle_load(v_file_name):
    return pickle.load(open(v_file_name, "rb"))


def get_oracle_table_from_dbm(sql_text):
    if 'connection_oracle_ali' not in globals():
        print('connection does not exist. Try to connect it...')
        f = open('ali_oracle_dbm_conn_string.txt', "r")
        fx = f.read()
        ####
        global connection_oracle_ali
        connection_oracle_ali = cx_Oracle.connect(fx)
        ####
        print('connection established!!')

    print('Already have connection. Just fetch data!!')
    return pd.read_sql(sql_text, con=connection_oracle_ali)


# Write pandas.DataFrame object to an excel file.
def write_to_excel_file_by_pandas(excel_file_path, data_frame, sheet_name="data"):
    excel_writer = pd.ExcelWriter(excel_file_path, engine='xlsxwriter')
    data_frame.to_excel(excel_writer, sheet_name)
    excel_writer.save()
    print(excel_file_path + ' has been created.')


def f_get_upperlower_limits_IQR(v_series, v_iqr_multiplier=1.5):
    '''
    v_series : it has to be time series with value column as numeric
    v_iqr_multiplier : it is IQR multiplier. Default value=1.5
    it returns lower and upper limits wrt to IQR
    '''
    q3 = np.quantile(v_series, 0.75)
    q1 = np.quantile(v_series, 0.25)
    IQR = q3 - q1
    upper = q3 + (v_iqr_multiplier * IQR)
    lower = q1 - (v_iqr_multiplier * IQR)
    return lower, upper


def f_get_upperlower_limits_IQR_MC(v_series):
    '''
    it just takes series and returns MC, upper and lower limits wrt medcouple IQR method
    #v_tolerance =           Q1     Q3
                    x < 0    -3     4
                    x > 0    -4     3
    v_tolerance = [[-3, 4],[-4,3]]
    '''
    if len(v_series) > 30000:
        v_series = v_series.sample(30000)

    MC = medcouple(v_series)
    q3 = np.quantile(v_series, 0.75)
    q1 = np.quantile(v_series, 0.25)
    IQR = q3 - q1

    if MC <= 0:
        lower = q1 - (1.5 * np.exp(-3 * MC) * IQR)
        upper = q3 + (1.5 * np.exp(4 * MC) * IQR)
    else:
        lower = q1 - (1.5 * np.exp(-4 * MC) * IQR)
        upper = q3 + (1.5 * np.exp(3 * MC) * IQR)

    return MC, lower, upper


# v_df as timeseries. like ( f_plot_hist(data_df["VAL"], 100) )
def f_plot_hist(v_df, v_binsize=20, v_title=None):
    '''
    just pass series to this function, it shows histogram
    '''
    plt.figure(figsize=(10, 10))
    sns.set_style("whitegrid")
    plot_data = v_df
    plot_data.hist(bins=v_binsize)
    # sns.distplot( plot_data)
    plt.xticks(np.linspace(np.min(plot_data), np.max(plot_data), v_binsize), rotation=45)
    if v_title is not None:
        plt.title(v_title)
    plt.show()


def f_plot_seaborn_barplot(v_df, v_x, v_hue=None, v_xticks_rotation=45, v_title=None):
    '''
    v_df: dataframe
    v_x: x1 axis dimension, should be categorical
    v_hue: x2 axis dimension, should be categorical (it might be null)
    it d
    '''
    plt.figure(figsize=(10, 10))

    g = sns.countplot(data=v_df,
                      x=v_x,
                      hue=None if v_hue == None else v_hue,
                      order=v_df[v_x].value_counts().index
                      )

    for item in g.get_xticklabels():
        item.set_rotation(45 if v_xticks_rotation is None else v_xticks_rotation)

    for i in g.patches:
        g.text(i.get_x() + 0.1  # x ekseninde nerde
               , 0 if np.isnan(i.get_height()) == True else i.get_height() + 0.2  # y ekseninde nerde
               , str(0 if np.isnan(i.get_height()) == True else i.get_height())
               , fontsize=10
               , color='dimgrey')

    g.legend(loc="upper right")

    if v_title is not None:
        plt.title(v_title)


def f_plot_bar_series(v_series
                      , v_bar_tp='bar'
                      , v_color="blue"
                      , v_title=None
                      , v_xlabel=None
                      , v_show_percent=True
                      ):
    '''
    v_series: Must be series.like:
                      Man 1000
                      Woman 3000
              Also it has to be already grouped by object
    '''
    if v_bar_tp == 'bar':

        fig, ax = plt.subplots()
        #        fig = plt.figure(figsize=(10,10))
        #        ax = fig.add_subplot(111)

        ax = v_series.plot(kind=v_bar_tp
                           , figsize=(10, 15)
                           , color=v_color
                           , fontsize=13);
        ax.set_alpha(0.8)
        if v_title is not None:
            ax.set_title(v_title, fontsize=12)

        if v_xlabel is not None:
            ax.set_xlabel(v_xlabel, fontsize=10);
        # ax.set_xticks([0, 5, 10, 15, 20])

        # create a list to collect the plt.patches data
        totals = []

        # find the values and append to list
        for i in ax.patches:
            totals.append(i.get_height())

        # set individual bar lables using above list
        total = sum(totals)

        # set individual bar lables using above list
        for i in ax.patches:
            # get_width pulls left or right; get_y pushes up or down
            if v_show_percent == True:
                x_text = str(i.get_width()) + ' (' + str(round((i.get_width() / float(total)) * 100, 2)) + '%)'
            else:
                x_text = str(i.get_width())
            ax.text(i.get_x() + 0.1  # x ekseninde nerde
                    , i.get_height() + 0.2  # y ekseninde nerde
                    , x_text
                    , fontsize=15
                    , color='dimgrey')
        plt.show()

    if v_bar_tp == 'barh':

        fig, ax = plt.subplots()

        my_colors = ['r', 'g', 'b', 'k', 'y', 'm', 'c']

        ax = v_series.plot(kind=v_bar_tp
                           , figsize=(10, 10)
                           , color=my_colors
                           , fontsize=13);
        ax.set_alpha(0.8)
        if v_title is not None:
            ax.set_title(v_title, fontsize=12)

        if v_xlabel is not None:
            ax.set_xlabel(v_xlabel, fontsize=10);
        # ax.set_xticks([0, 5, 10, 15, 20])

        # create a list to collect the plt.patches data
        totals = []

        # find the values and append to list
        for i in ax.patches:
            totals.append(i.get_width())

        # set individual bar lables using above list
        total = sum(totals)

        # set individual bar lables using above list
        for i in ax.patches:
            # get_width pulls left or right; get_y pushes up or down
            if v_show_percent == True:
                x_text = str(i.get_width()) + ' (' + str(round((i.get_width() / float(total)) * 100, 2)) + '%)'
            else:
                x_text = str(i.get_width())
            ax.text(i.get_width() + 0.1  # x ekseninde nerde
                    , i.get_y() + 0.2  # y ekseninde nerde
                    #                   ,str(i.get_width()) + ' (' + str(round((i.get_width()/float(total))*100, 2)) +'%)'
                    , x_text
                    , fontsize=15
                    , color='dimgrey')
        plt.show()


def f_generate_bin_by_percents(v_series, v_num_of_bins):
    '''
    v_series: series wants to be seperated into bins
    v_num_of_bins: number of bins that user wants to have. It should be divided to 1!! For example num_of 3 bins is not valid
    Returns: List of bins
    '''
    inc = 1.0 / v_num_of_bins
    start_quantile = 0
    list_bin = []
    for i in range(v_num_of_bins + 1):
        list_bin.append(np.quantile(v_series, round(start_quantile, 5)))
        start_quantile = start_quantile + inc
    return list_bin


def f_apply_chisquare_test(v1_series, v2_series):
    '''
    v1_series: first categoric series
    v2_series: second categoric series
    it returns chi square test results and brief explanation.
    '''
    print("\n")
    np.random.seed(200)
    #    cont_matrix = np.sqrt( pd.crosstab(v1_series , v2_series) )
    cont_matrix = pd.crosstab(v1_series, v2_series)
    chi2_stat, p_val, dof, ex = stats.chi2_contingency(cont_matrix)
    prob = 0.95
    critical = stats.chi2.ppf(prob, dof)

    df_delta = pd.DataFrame(data=np.round(((cont_matrix.values - ex) / ex) * 100, 2), columns=cont_matrix.columns,
                            index=cont_matrix.index)

    for col in cont_matrix.columns:
        df_delta[col] = df_delta[col].apply(lambda x: str(x) + '%')

    print("===Real Values===")
    print(cont_matrix)

    print("\n")

    print("===Exp Values===")
    print(pd.DataFrame(data=ex, columns=cont_matrix.columns, index=cont_matrix.index))

    print("\n")

    print("===Delta Values===")
    print(df_delta)

    print("\n")

    print("===Chi2 Stat===")
    print(chi2_stat)
    print("\n")
    print("===Critical===")
    print(critical)
    print("\n")
    print("===Degrees of Freedom===")
    print(dof)
    print("\n")
    print("===P-Value===")
    print(p_val)
    print("\n")

    print("NULL Hypothesis says that: they are independent. So if p<0.05 then we can say they depend on each other")
    print(
        "NULL Hypothesis says that: they are independent. So if chi2_stat>=critical then we can say they depend on each other")

    print("\n")

    if p_val < 0.05:
        print("***** Result is: They Depend on each other with 95% confidence level")
    else:
        print("***** Result is: They Don't Depend on each other with 95% confidence level")

    print("\n")

    if abs(chi2_stat) >= critical:
        print("***** Result is: They Depend on each other according to stats")
    else:
        print("***** Result is: They Don't Depend on each other according to stats")


def f_create_newbins_of_percent(v_dfx, v_percent):
    '''
    v_dfx: series just index for dimension and metric
    v_percent: cutting percentage point (like 95)
    it returns a tuple:
        [0] = a list showing cutting points
        [1] = new series of which metric column is replaced by cutting points
    If one wants to cut immediately decreased population at a specific threshold like:
                  percent  cumulative percent
        0 trx --> %60      %60
        1 trx --> %30      %90
        2 trx --> %3       %93
        3 trx --> %2       %95
        4 trx --> %0.5     %95.5
        5 trx --> %0.4     %95.9
        6 trx --> %0.2     %96.1
        7 trx --> %0.1     %96.2
        ........................
    function just cuts data at desired percentage.
    For instance if you want +95%:
        count column replaced with the values of (0, 1, 2, 3+) for this example
    For instance if you want +90%:
        count column replaced with the values of (0, 1+) for this example


    '''

    v_df = v_dfx.copy()

    tot_length = len(v_df)

    df_val_counts = pd.DataFrame(v_df[v_df.columns[0]].value_counts())
    df_val_counts.reset_index(inplace=True)

    v_list = []
    for i in df_val_counts.index.unique():
        if float(len(v_df[v_df[v_df.columns[0]] <= int(i)])) / tot_length >= float(v_percent) / 100:
            v_list.append(i)
            break;
        v_list.append(i)

    v_df[v_df.columns[0]] = v_df[v_df.columns[0]].apply(lambda x: str(v_list[-1]) + '+' if x >= v_list[-1] else str(x))

    return v_list, v_df


def f_create_conf_matrix(v_clf, X_test, y_test):
    #    y_true, y_pred, y_pred_prob = y_test, v_clf.predict(X_test),  v_clf.predict_proba(X_test)
    print(classification_report(y_true, y_pred))
    print(confusion_matrix(y_true, y_pred))


def f_create_test_table(predictor, X_test, y_test, thr_value=0.5):
    predictions = predictor.predict_proba(X_test)

    final_table = pd.DataFrame(data=predictions, columns=[str(x) + "_prob" for x in predictor.classes_],
                               index=y_test.index)
    final_table["final_prob"] = final_table.apply(np.max, axis=1)
    final_table["PRED"] = final_table.idxmax(axis=1).apply(lambda x: x[0])
    final_table["ACTUAL"] = y_test.values
    final_table["FINAL_PRED"] = final_table.apply(
        lambda row: int(-99) if row["final_prob"] < thr_value else int(row["PRED"]), axis=1)

    part_len = len(final_table[final_table["FINAL_PRED"] == -99])
    full_len = len(final_table)
    print(part_len)
    print(full_len)
    print("%" + str(float(part_len) / full_len * 100) + " is lossed")

    print(classification_report(final_table[final_table["FINAL_PRED"] != -99]["ACTUAL"],
                                final_table[final_table["FINAL_PRED"] != -99]["FINAL_PRED"]))
    print(confusion_matrix(final_table[final_table["FINAL_PRED"] != -99]["ACTUAL"],
                           final_table[final_table["FINAL_PRED"] != -99]["FINAL_PRED"]))

    return final_table


def f_create_similarity (v_df, v_ref_index, v_numeric_columns_list, v_categoric_column_list):
    from sklearn.preprocessing import StandardScaler
    from sklearn.preprocessing import MinMaxScaler
    import sklearn.metrics as sm
    # v_df = tfidf_df.copy()
    # v_numeric_columns_list = [x for x in v_df.columns if x not in ["target_seviye"]]
    # v_categoric_column_list = []
    # v_ref_index = 'SD10653794'
    scaler = StandardScaler().fit(v_df[v_numeric_columns_list])
    df_scaled_numeric = pd.DataFrame(data = scaler.transform(v_df[v_numeric_columns_list]) , columns = v_numeric_columns_list, index = v_df.index)
    ref_array = df_scaled_numeric.loc[v_ref_index, :].values
    ref_array_cat = v_df.loc[v_ref_index, v_categoric_column_list].values
    list_dummy = []
    for i_index in df_scaled_numeric.index:
        dist_euk = 0
        if len(v_numeric_columns_list) != 0:
            df_scaled_numeric.loc[i_index,:].values
            dist_euk = np.linalg.norm(ref_array - df_scaled_numeric.loc[i_index,:].values)

        dist_hamming = 0
        if len(v_categoric_column_list) != 0:
            try:
#                do_something() # custom operation
                compare_array_cat = v_df.loc[i_index, v_categoric_column_list].values
                dist_hamming = sm.hamming_loss(compare_array_cat, ref_array_cat)
            except BaseException as e:
                print("index:" + i_index)
                print(ref_array_cat)
                print(compare_array_cat)
                raise ValueError("EXIT!!! " + str(e))

        list_dummy.append([v_ref_index, i_index, dist_euk, dist_hamming])

    numeric_sim_df = pd.DataFrame(data = list_dummy, columns = ["ref_index", "compare_index", "euk_dist", "hamm_dist"])

    numeric_sim_df["euk_dist_norm"] = MinMaxScaler().fit_transform(numeric_sim_df[["euk_dist"]])
    numeric_sim_df["tot_distance"] = numeric_sim_df.apply( lambda row: ((row["euk_dist_norm"] * len(v_numeric_columns_list))
                                                                         +
                                                                         (row["hamm_dist"] * len(v_categoric_column_list))
                                                                        )
                                                                        /
                                                                        (len(v_numeric_columns_list) + len(v_categoric_column_list)), axis=1)

    numeric_sim_df = numeric_sim_df.sort_values("tot_distance", ascending=True)
    return numeric_sim_df
